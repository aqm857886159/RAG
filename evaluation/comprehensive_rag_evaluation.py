#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
RAGç³»ç»Ÿç»¼åˆè¯„ä¼°è„šæœ¬
åŸºäºæŠ€æœ¯æ–‡æ¡£è¿›è¡Œå…¨é¢çš„RAGåŠŸèƒ½æµ‹è¯•å’Œæ€§èƒ½è¯„ä¼°
"""

import os
import sys
import logging
import time
import json
from pathlib import Path
from typing import List, Dict, Any
import statistics

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# è®¾ç½®APIå¯†é’¥
os.environ["DEEPSEEK_API_KEY"] = "sk-06810fb5453e4fd1b39e3e5f566da210"

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RAGSystemEvaluator:
    """RAGç³»ç»Ÿç»¼åˆè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.test_document = "Untitled.txt"
        self.evaluation_results = {}
        self.test_queries = [
            # åŸºç¡€ä¿¡æ¯æŸ¥è¯¢
            "ä»€ä¹ˆæ˜¯Streamlitï¼Ÿå®ƒæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
            "è¿™ä¸ªé¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æ–‡æ¡£æåˆ°äº†å“ªäº›ä¸»è¦çš„æŠ€æœ¯æ ˆï¼Ÿ",
            
            # æŠ€æœ¯ç»†èŠ‚æŸ¥è¯¢
            "Unstructured.ioæœ‰å“ªäº›å…³é”®ç‰¹æ€§ï¼Ÿ",
            "PDFè§£ææ¨èä½¿ç”¨å“ªäº›åº“ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ", 
            "æ–‡æœ¬åˆ†å—æœ‰å“ªäº›ç­–ç•¥ï¼Ÿå„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ",
            
            # å¯¹æ¯”åˆ†ææŸ¥è¯¢
            "PyMuPDFå’ŒPDFPlumberçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å›ºå®šå¤§å°åˆ†å—å’Œè¯­ä¹‰åˆ†å—çš„åŒºåˆ«ï¼Ÿ",
            "OCRå¼•æ“ä¸­PaddleOCRå’ŒTesseractæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ",
            
            # å®æ–½æ–¹æ¡ˆæŸ¥è¯¢
            "é¡¹ç›®å®æ–½åˆ†ä¸ºå“ªå‡ ä¸ªé˜¶æ®µï¼Ÿæ¯ä¸ªé˜¶æ®µçš„é‡ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "éƒ¨ç½²æœ‰å“ªäº›æ–¹æ¡ˆé€‰æ‹©ï¼Ÿ",
            "å¦‚ä½•å¤„ç†APIå¯†é’¥ç®¡ç†ï¼Ÿ",
            
            # æŠ€æœ¯æ¶æ„æŸ¥è¯¢
            "ç³»ç»Ÿçš„é«˜å±‚æ¶æ„åŒ…å«å“ªäº›å±‚ï¼Ÿ",
            "UniversalDocumentProcessoré‡‡ç”¨äº†ä»€ä¹ˆè®¾è®¡æ¨¡å¼ï¼Ÿ",
            "å¼‚æ­¥å¤„ç†æœ‰å“ªäº›æ–¹æ¡ˆï¼Ÿ"
        ]
        
    def load_test_document(self) -> bool:
        """åŠ è½½æµ‹è¯•æ–‡æ¡£"""
        print("\n" + "="*70)
        print("ğŸ“„ ç¬¬ä¸€æ­¥ï¼šæ–‡æ¡£åŠ è½½æµ‹è¯•")
        print("="*70)
        
        try:
            from document_loader import DocumentLoader
            
            loader = DocumentLoader()
            
            if not Path(self.test_document).exists():
                print(f"âŒ æµ‹è¯•æ–‡æ¡£ä¸å­˜åœ¨: {self.test_document}")
                return False
                
            start_time = time.time()
            docs = loader.load_single_document(self.test_document)
            load_time = time.time() - start_time
            
            print(f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸ")
            print(f"ğŸ“Š æ–‡æ¡£ç‰‡æ®µæ•°: {len(docs)}")
            print(f"â±ï¸ åŠ è½½è€—æ—¶: {load_time:.2f} ç§’")
            
            if docs:
                content_length = len(docs[0].page_content)
                print(f"ğŸ“„ æ–‡æ¡£å†…å®¹é•¿åº¦: {content_length:,} å­—ç¬¦")
                print(f"ğŸ“ å†…å®¹é¢„è§ˆ: {docs[0].page_content[:200]}...")
                
            self.evaluation_results["document_loading"] = {
                "status": "success",
                "document_count": len(docs),
                "load_time": load_time,
                "content_length": content_length if docs else 0
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
            self.evaluation_results["document_loading"] = {
                "status": "failed",
                "error": str(e)
            }
            return False
    
    def test_text_chunking(self) -> bool:
        """æµ‹è¯•æ–‡æœ¬åˆ†å—åŠŸèƒ½"""
        print("\n" + "="*70)
        print("âœ‚ï¸ ç¬¬äºŒæ­¥ï¼šæ–‡æœ¬åˆ†å—æµ‹è¯•")
        print("="*70)
        
        try:
            from document_loader import DocumentLoader
            from text_splitter import get_text_splitter
            
            loader = DocumentLoader()
            docs = loader.load_single_document(self.test_document)
            
            # æµ‹è¯•ä¸åŒåˆ†å—ç­–ç•¥
            strategies = [
                ("recursive", {"chunk_size": 500, "chunk_overlap": 50}),
                ("recursive", {"chunk_size": 1000, "chunk_overlap": 100}),
                ("character", {"chunk_size": 500, "chunk_overlap": 50})
            ]
            
            chunking_results = {}
            
            for strategy_name, params in strategies:
                print(f"\nğŸ”„ æµ‹è¯• {strategy_name} åˆ†å—ç­–ç•¥ (chunk_size={params['chunk_size']})")
                
                start_time = time.time()
                splitter = get_text_splitter(strategy_name, **params)
                chunks = splitter.split_documents(docs)
                chunk_time = time.time() - start_time
                
                # è®¡ç®—åˆ†å—è´¨é‡æŒ‡æ ‡
                chunk_lengths = [len(chunk.page_content) for chunk in chunks]
                avg_length = statistics.mean(chunk_lengths)
                length_std = statistics.stdev(chunk_lengths) if len(chunk_lengths) > 1 else 0
                
                print(f"   âœ… ç”Ÿæˆæ–‡æœ¬å—: {len(chunks)} ä¸ª")
                print(f"   ğŸ“Š å¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
                print(f"   ğŸ“ˆ é•¿åº¦æ ‡å‡†å·®: {length_std:.0f}")
                print(f"   â±ï¸ åˆ†å—è€—æ—¶: {chunk_time:.2f} ç§’")
                
                # æ˜¾ç¤ºç¤ºä¾‹å—
                if chunks:
                    print(f"   ğŸ“ ç¤ºä¾‹å—: {chunks[0].page_content[:150]}...")
                
                chunking_results[f"{strategy_name}_{params['chunk_size']}"] = {
                    "chunk_count": len(chunks),
                    "avg_length": avg_length,
                    "length_std": length_std,
                    "chunk_time": chunk_time
                }
            
            self.evaluation_results["text_chunking"] = {
                "status": "success",
                "strategies_tested": len(strategies),
                "results": chunking_results
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ æ–‡æœ¬åˆ†å—æµ‹è¯•å¤±è´¥: {str(e)}")
            self.evaluation_results["text_chunking"] = {
                "status": "failed", 
                "error": str(e)
            }
            return False
    
    def test_intent_recognition(self) -> bool:
        """æµ‹è¯•æ„å›¾è¯†åˆ«å‡†ç¡®æ€§"""
        print("\n" + "="*70) 
        print("ğŸ§  ç¬¬ä¸‰æ­¥ï¼šæ„å›¾è¯†åˆ«æµ‹è¯•")
        print("="*70)
        
        try:
            from intent_recognizer import get_intent_recognizer
            
            recognizer = get_intent_recognizer(llm_provider="deepseek")
            
            intent_results = []
            total_time = 0
            
            # é¢„æœŸæ„å›¾åˆ†ç±»ï¼ˆç”¨äºè¯„ä¼°å‡†ç¡®æ€§ï¼‰
            expected_intents = {
                "ä»€ä¹ˆæ˜¯Streamlitï¼Ÿå®ƒæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ": "ä¿¡æ¯æŸ¥è¯¢",
                "è¿™ä¸ªé¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ": "ä¿¡æ¯æŸ¥è¯¢", 
                "PyMuPDFå’ŒPDFPlumberçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ": "æ¯”è¾ƒç±»é—®é¢˜",
                "å›ºå®šå¤§å°åˆ†å—å’Œè¯­ä¹‰åˆ†å—çš„åŒºåˆ«ï¼Ÿ": "æ¯”è¾ƒç±»é—®é¢˜",
                "é¡¹ç›®å®æ–½åˆ†ä¸ºå“ªå‡ ä¸ªé˜¶æ®µï¼Ÿæ¯ä¸ªé˜¶æ®µçš„é‡ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ": "æ“ä½œæŒ‡å—",
                "å¦‚ä½•å¤„ç†APIå¯†é’¥ç®¡ç†ï¼Ÿ": "æ“ä½œæŒ‡å—"
            }
            
            correct_predictions = 0
            
            for query in expected_intents.keys():
                print(f"\nğŸ”„ æµ‹è¯•æŸ¥è¯¢: {query[:50]}...")
                
                start_time = time.time()
                result = recognizer.recognize_intent(query)
                intent_time = time.time() - start_time
                total_time += intent_time
                
                predicted_intent = result['intent']
                expected_intent = expected_intents[query]
                is_correct = predicted_intent == expected_intent
                
                if is_correct:
                    correct_predictions += 1
                    print(f"   âœ… é¢„æµ‹æ­£ç¡®: {predicted_intent} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
                else:
                    print(f"   âš ï¸ é¢„æµ‹é”™è¯¯: {predicted_intent} (æœŸæœ›: {expected_intent}, ç½®ä¿¡åº¦: {result['confidence']:.3f})")
                
                intent_results.append({
                    "query": query,
                    "predicted": predicted_intent,
                    "expected": expected_intent,
                    "correct": is_correct,
                    "confidence": result['confidence'],
                    "time": intent_time
                })
            
            accuracy = correct_predictions / len(expected_intents)
            avg_time = total_time / len(expected_intents)
            avg_confidence = statistics.mean([r['confidence'] for r in intent_results])
            
            print(f"\nğŸ“Š æ„å›¾è¯†åˆ«è¯„ä¼°ç»“æœ:")
            print(f"   ğŸ¯ å‡†ç¡®ç‡: {accuracy:.2%} ({correct_predictions}/{len(expected_intents)})")
            print(f"   â±ï¸ å¹³å‡è€—æ—¶: {avg_time:.2f} ç§’/æ¬¡")
            print(f"   ğŸ“ˆ å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
            
            self.evaluation_results["intent_recognition"] = {
                "status": "success",
                "accuracy": accuracy,
                "correct_predictions": correct_predictions,
                "total_queries": len(expected_intents),
                "avg_time": avg_time,
                "avg_confidence": avg_confidence,
                "detailed_results": intent_results
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ æ„å›¾è¯†åˆ«æµ‹è¯•å¤±è´¥: {str(e)}")
            self.evaluation_results["intent_recognition"] = {
                "status": "failed",
                "error": str(e)
            }
            return False
    
    def test_qa_generation(self) -> bool:
        """æµ‹è¯•é—®ç­”å¯¹ç”Ÿæˆè´¨é‡"""
        print("\n" + "="*70)
        print("ğŸ’¬ ç¬¬å››æ­¥ï¼šé—®ç­”å¯¹ç”Ÿæˆæµ‹è¯•")
        print("="*70)
        
        try:
            from data_converter import DocumentConverter
            from document_loader import DocumentLoader
            
            # åŠ è½½æ–‡æ¡£å¹¶è½¬æ¢
            loader = DocumentLoader()
            docs = loader.load_single_document(self.test_document)
            
            converter = DocumentConverter(
                llm_provider="deepseek",
                api_key="sk-06810fb5453e4fd1b39e3e5f566da210"
            )
            
            # åˆ†å‰²æ–‡æ¡£ï¼ˆå–å‰å‡ ä¸ªå—è¿›è¡Œæµ‹è¯•ä»¥èŠ‚çœæ—¶é—´ï¼‰
            chunks = converter.text_splitter.split_documents(docs)
            test_chunks = chunks[:3]  # åªæµ‹è¯•å‰3ä¸ªå—
            
            print(f"ğŸ”„ å¯¹ {len(test_chunks)} ä¸ªæ–‡æœ¬å—è¿›è¡Œé—®ç­”å¯¹ç”Ÿæˆæµ‹è¯•...")
            
            start_time = time.time()
            qa_pairs = converter._generate_qa_pairs(test_chunks)
            generation_time = time.time() - start_time
            
            print(f"âœ… é—®ç­”å¯¹ç”Ÿæˆå®Œæˆ")
            print(f"ğŸ“Š ç”Ÿæˆæ•°é‡: {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
            print(f"â±ï¸ ç”Ÿæˆè€—æ—¶: {generation_time:.2f} ç§’")
            print(f"ğŸ“ˆ ç”Ÿæˆæ•ˆç‡: {len(qa_pairs)/generation_time:.1f} ä¸ª/ç§’")
            
            # æ˜¾ç¤ºç”Ÿæˆçš„é—®ç­”å¯¹ç¤ºä¾‹
            print(f"\nğŸ“ ç”Ÿæˆçš„é—®ç­”å¯¹ç¤ºä¾‹:")
            for i, qa in enumerate(qa_pairs[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"\n   Q{i+1}: {qa.question}")
                print(f"   A{i+1}: {qa.answer[:150]}{'...' if len(qa.answer) > 150 else ''}")
            
            # è¯„ä¼°é—®ç­”å¯¹è´¨é‡ï¼ˆåŸºäºç®€å•æŒ‡æ ‡ï¼‰
            avg_question_length = statistics.mean([len(qa.question) for qa in qa_pairs])
            avg_answer_length = statistics.mean([len(qa.answer) for qa in qa_pairs])
            
            self.evaluation_results["qa_generation"] = {
                "status": "success",
                "qa_pairs_count": len(qa_pairs),
                "generation_time": generation_time,
                "generation_rate": len(qa_pairs)/generation_time,
                "avg_question_length": avg_question_length,
                "avg_answer_length": avg_answer_length,
                "sample_qa_pairs": [
                    {"question": qa.question, "answer": qa.answer} 
                    for qa in qa_pairs[:2]
                ]
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ é—®ç­”å¯¹ç”Ÿæˆæµ‹è¯•å¤±è´¥: {str(e)}")
            self.evaluation_results["qa_generation"] = {
                "status": "failed",
                "error": str(e)
            }
            return False
    
    def test_vector_retrieval(self) -> bool:
        """æµ‹è¯•å‘é‡æ£€ç´¢ç›¸å…³æ€§"""
        print("\n" + "="*70)
        print("ğŸ” ç¬¬äº”æ­¥ï¼šå‘é‡æ£€ç´¢æµ‹è¯•")
        print("="*70)
        
        try:
            from vectorizer import SimpleMemoryVectorStore
            from sentence_transformers import SentenceTransformer
            from document_loader import DocumentLoader
            from text_splitter import get_text_splitter
            
            # åˆ›å»ºé€‚é…å™¨
            class SentenceTransformerAdapter:
                def __init__(self, model):
                    self.model = model
                    
                def embed_documents(self, texts):
                    return self.model.encode(texts).tolist()
                    
                def embed_query(self, text):
                    return self.model.encode([text])[0].tolist()
            
            # åˆå§‹åŒ–ç»„ä»¶
            print("ğŸ”„ åˆå§‹åŒ–å‘é‡å­˜å‚¨ç³»ç»Ÿ...")
            base_model = SentenceTransformer('all-MiniLM-L6-v2')
            embeddings = SentenceTransformerAdapter(base_model)
            vector_store = SimpleMemoryVectorStore(embeddings)
            
            # åŠ è½½å’Œåˆ†å—æ–‡æ¡£
            loader = DocumentLoader()
            docs = loader.load_single_document(self.test_document)
            splitter = get_text_splitter("recursive", chunk_size=500, chunk_overlap=50)
            chunks = splitter.split_documents(docs)
            
            # æ„å»ºå‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨å‰20ä¸ªå—é¿å…è¿‡é•¿ï¼‰
            test_chunks = chunks[:20]
            print(f"ğŸ”„ æ„å»ºå‘é‡å­˜å‚¨ ({len(test_chunks)} ä¸ªæ–‡æœ¬å—)...")
            
            build_start = time.time()
            vector_store.add_documents(test_chunks)
            build_time = time.time() - build_start
            
            print(f"âœ… å‘é‡å­˜å‚¨æ„å»ºå®Œæˆï¼Œè€—æ—¶ {build_time:.2f} ç§’")
            
            # æµ‹è¯•æ£€ç´¢ç›¸å…³æ€§
            test_queries_for_retrieval = [
                "Streamlitçš„ç‰¹ç‚¹å’Œä¼˜åŠ¿",
                "æ–‡æ¡£è§£æçš„æŠ€æœ¯é€‰å‹", 
                "æ–‡æœ¬åˆ†å—ç­–ç•¥æ¯”è¾ƒ",
                "éƒ¨ç½²æ–¹æ¡ˆé€‰æ‹©"
            ]
            
            retrieval_results = []
            total_retrieval_time = 0
            
            for query in test_queries_for_retrieval:
                print(f"\nğŸ” æ£€ç´¢æŸ¥è¯¢: {query}")
                
                start_time = time.time()
                results = vector_store.similarity_search(query, k=3)
                search_time = time.time() - start_time
                total_retrieval_time += search_time
                
                print(f"   âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œè€—æ—¶ {search_time:.3f} ç§’")
                
                # æ˜¾ç¤ºæœ€ç›¸å…³çš„ç»“æœ
                if results:
                    print(f"   ğŸ“„ æœ€ç›¸å…³å†…å®¹: {results[0].page_content[:100]}...")
                
                retrieval_results.append({
                    "query": query,
                    "results_count": len(results),
                    "search_time": search_time,
                    "top_result": results[0].page_content[:200] if results else ""
                })
            
            avg_retrieval_time = total_retrieval_time / len(test_queries_for_retrieval)
            
            print(f"\nğŸ“Š å‘é‡æ£€ç´¢è¯„ä¼°ç»“æœ:")
            print(f"   ğŸ“š æ–‡æ¡£åº“å¤§å°: {len(test_chunks)} ä¸ªæ–‡æ¡£å—")
            print(f"   ğŸ—ï¸ æ„å»ºè€—æ—¶: {build_time:.2f} ç§’")
            print(f"   ğŸ” å¹³å‡æ£€ç´¢è€—æ—¶: {avg_retrieval_time:.3f} ç§’/æ¬¡")
            print(f"   âš¡ æ£€ç´¢é€Ÿåº¦: {len(test_chunks)/avg_retrieval_time:.0f} æ–‡æ¡£/ç§’")
            
            self.evaluation_results["vector_retrieval"] = {
                "status": "success",
                "document_count": len(test_chunks),
                "build_time": build_time,
                "avg_retrieval_time": avg_retrieval_time,
                "queries_tested": len(test_queries_for_retrieval),
                "retrieval_results": retrieval_results
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ å‘é‡æ£€ç´¢æµ‹è¯•å¤±è´¥: {str(e)}")
            self.evaluation_results["vector_retrieval"] = {
                "status": "failed",
                "error": str(e)
            }
            return False
    
    def test_end_to_end_rag(self) -> bool:
        """æµ‹è¯•ç«¯åˆ°ç«¯RAGé—®ç­”"""
        print("\n" + "="*70)
        print("ğŸ¤– ç¬¬å…­æ­¥ï¼šç«¯åˆ°ç«¯RAGé—®ç­”æµ‹è¯•")
        print("="*70)
        
        try:
            from generator import get_generator
            from src.llm import get_llm
            from vectorizer import SimpleMemoryVectorStore
            from sentence_transformers import SentenceTransformer
            from document_loader import DocumentLoader
            from text_splitter import get_text_splitter
            
            # åˆå§‹åŒ–å®Œæ•´RAGç³»ç»Ÿ
            print("ğŸ”„ åˆå§‹åŒ–å®Œæ•´RAGç³»ç»Ÿ...")
            
            # åˆ›å»ºé€‚é…å™¨
            class SentenceTransformerAdapter:
                def __init__(self, model):
                    self.model = model
                def embed_documents(self, texts):
                    return self.model.encode(texts).tolist()
                def embed_query(self, text):
                    return self.model.encode([text])[0].tolist()
            
            # æ„å»ºçŸ¥è¯†åº“
            loader = DocumentLoader()
            docs = loader.load_single_document(self.test_document)
            splitter = get_text_splitter("recursive", chunk_size=500, chunk_overlap=50)
            chunks = splitter.split_documents(docs)
            
            # å‘é‡å­˜å‚¨
            base_model = SentenceTransformer('all-MiniLM-L6-v2')
            embeddings = SentenceTransformerAdapter(base_model)
            vector_store = SimpleMemoryVectorStore(embeddings)
            vector_store.add_documents(chunks[:15])  # ä½¿ç”¨å‰15ä¸ªå—
            
            # åˆ›å»ºRAGç”Ÿæˆå™¨
            llm = get_llm(provider="deepseek", api_key="sk-06810fb5453e4fd1b39e3e5f566da210")
            rag_generator = get_generator(use_rag=True, llm=llm, vector_store=vector_store)
            
            print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
            # æµ‹è¯•æŸ¥è¯¢
            test_rag_queries = [
                "Streamlitæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿä¸ºä»€ä¹ˆé€‰æ‹©å®ƒï¼Ÿ",
                "PDFè§£ææ¨èä½¿ç”¨å“ªä¸ªåº“ï¼ŸåŸå› æ˜¯ä»€ä¹ˆï¼Ÿ",
                "é¡¹ç›®å®æ–½åˆ†ä¸ºå“ªäº›é˜¶æ®µï¼Ÿ"
            ]
            
            rag_results = []
            total_rag_time = 0
            
            for query in test_rag_queries:
                print(f"\nâ“ é—®é¢˜: {query}")
                
                start_time = time.time()
                result = rag_generator.generate(query)
                rag_time = time.time() - start_time
                total_rag_time += rag_time
                
                answer = result.get('answer', 'æœªç”Ÿæˆå›ç­”')
                source_docs = result.get('source_documents', [])
                
                print(f"   âœ… å›ç­”ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {rag_time:.2f} ç§’")
                print(f"   ğŸ“š ä½¿ç”¨äº† {len(source_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
                print(f"   ğŸ’¬ å›ç­”: {answer[:200]}{'...' if len(answer) > 200 else ''}")
                
                rag_results.append({
                    "query": query,
                    "answer": answer,
                    "source_count": len(source_docs),
                    "response_time": rag_time,
                    "answer_length": len(answer)
                })
            
            avg_rag_time = total_rag_time / len(test_rag_queries)
            avg_answer_length = statistics.mean([r['answer_length'] for r in rag_results])
            
            print(f"\nğŸ“Š ç«¯åˆ°ç«¯RAGè¯„ä¼°ç»“æœ:")
            print(f"   ğŸ¯ æµ‹è¯•æŸ¥è¯¢æ•°: {len(test_rag_queries)}")
            print(f"   â±ï¸ å¹³å‡å“åº”æ—¶é—´: {avg_rag_time:.2f} ç§’/æ¬¡") 
            print(f"   ğŸ“ å¹³å‡å›ç­”é•¿åº¦: {avg_answer_length:.0f} å­—ç¬¦")
            print(f"   ğŸ“š çŸ¥è¯†åº“å¤§å°: {len(chunks[:15])} ä¸ªæ–‡æ¡£å—")
            
            self.evaluation_results["end_to_end_rag"] = {
                "status": "success",
                "queries_tested": len(test_rag_queries),
                "avg_response_time": avg_rag_time,
                "avg_answer_length": avg_answer_length,
                "knowledge_base_size": len(chunks[:15]),
                "detailed_results": rag_results
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ ç«¯åˆ°ç«¯RAGæµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            self.evaluation_results["end_to_end_rag"] = {
                "status": "failed",
                "error": str(e)
            }
            return False
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "="*70)
        print("ğŸ“Š RAGç³»ç»Ÿç»¼åˆè¯„ä¼°æŠ¥å‘Š")
        print("="*70)
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        passed_tests = 0
        total_tests = 0
        
        for test_name, result in self.evaluation_results.items():
            total_tests += 1
            if result.get("status") == "success":
                passed_tests += 1
        
        overall_score = passed_tests / total_tests if total_tests > 0 else 0
        
        # æ€§èƒ½æŒ‡æ ‡æ±‡æ€»
        performance_metrics = {}
        
        if "document_loading" in self.evaluation_results:
            doc_result = self.evaluation_results["document_loading"]
            if doc_result.get("status") == "success":
                performance_metrics["æ–‡æ¡£åŠ è½½"] = {
                    "è€—æ—¶": f"{doc_result.get('load_time', 0):.2f}ç§’",
                    "æ–‡æ¡£é•¿åº¦": f"{doc_result.get('content_length', 0):,}å­—ç¬¦"
                }
        
        if "intent_recognition" in self.evaluation_results:
            intent_result = self.evaluation_results["intent_recognition"] 
            if intent_result.get("status") == "success":
                performance_metrics["æ„å›¾è¯†åˆ«"] = {
                    "å‡†ç¡®ç‡": f"{intent_result.get('accuracy', 0):.1%}",
                    "å¹³å‡è€—æ—¶": f"{intent_result.get('avg_time', 0):.2f}ç§’",
                    "å¹³å‡ç½®ä¿¡åº¦": f"{intent_result.get('avg_confidence', 0):.3f}"
                }
        
        if "qa_generation" in self.evaluation_results:
            qa_result = self.evaluation_results["qa_generation"]
            if qa_result.get("status") == "success":
                performance_metrics["é—®ç­”ç”Ÿæˆ"] = {
                    "ç”Ÿæˆæ•°é‡": f"{qa_result.get('qa_pairs_count', 0)}ä¸ª",
                    "ç”Ÿæˆæ•ˆç‡": f"{qa_result.get('generation_rate', 0):.1f}ä¸ª/ç§’"
                }
        
        if "vector_retrieval" in self.evaluation_results:
            retrieval_result = self.evaluation_results["vector_retrieval"]
            if retrieval_result.get("status") == "success":
                performance_metrics["å‘é‡æ£€ç´¢"] = {
                    "æ£€ç´¢è€—æ—¶": f"{retrieval_result.get('avg_retrieval_time', 0):.3f}ç§’",
                    "æ–‡æ¡£åº“å¤§å°": f"{retrieval_result.get('document_count', 0)}ä¸ªå—"
                }
        
        if "end_to_end_rag" in self.evaluation_results:
            rag_result = self.evaluation_results["end_to_end_rag"]
            if rag_result.get("status") == "success":
                performance_metrics["ç«¯åˆ°ç«¯RAG"] = {
                    "å“åº”æ—¶é—´": f"{rag_result.get('avg_response_time', 0):.2f}ç§’",
                    "å›ç­”é•¿åº¦": f"{rag_result.get('avg_answer_length', 0):.0f}å­—ç¬¦"
                }
        
        # æ‰“å°æŠ¥å‘Š
        print(f"\nğŸ¯ æ€»ä½“è¯„ä¼°:")
        print(f"   æµ‹è¯•é€šè¿‡ç‡: {overall_score:.1%} ({passed_tests}/{total_tests})")
        
        if overall_score >= 0.8:
            print(f"   ç³»ç»ŸçŠ¶æ€: ğŸŸ¢ ä¼˜ç§€ - RAGç³»ç»Ÿè¿è¡ŒçŠ¶æ€è‰¯å¥½")
        elif overall_score >= 0.6:
            print(f"   ç³»ç»ŸçŠ¶æ€: ğŸŸ¡ è‰¯å¥½ - RAGç³»ç»ŸåŸºæœ¬å¯ç”¨ï¼Œéƒ¨åˆ†åŠŸèƒ½éœ€ä¼˜åŒ–")
        else:
            print(f"   ç³»ç»ŸçŠ¶æ€: ğŸ”´ éœ€æ”¹è¿› - RAGç³»ç»Ÿå­˜åœ¨è¾ƒå¤šé—®é¢˜")
        
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡æ±‡æ€»:")
        for category, metrics in performance_metrics.items():
            print(f"   {category}:")
            for metric, value in metrics.items():
                print(f"     - {metric}: {value}")
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        if "intent_recognition" in self.evaluation_results:
            intent_result = self.evaluation_results["intent_recognition"]
            if intent_result.get("status") == "success":
                accuracy = intent_result.get("accuracy", 0)
                if accuracy < 0.8:
                    recommendations.append("å»ºè®®ä¼˜åŒ–æ„å›¾è¯†åˆ«æç¤ºè¯ä»¥æé«˜å‡†ç¡®ç‡")
                if intent_result.get("avg_time", 0) > 5:
                    recommendations.append("æ„å›¾è¯†åˆ«å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–LLMè°ƒç”¨")
        
        if "vector_retrieval" in self.evaluation_results:
            retrieval_result = self.evaluation_results["vector_retrieval"]
            if retrieval_result.get("status") == "success":
                if retrieval_result.get("avg_retrieval_time", 0) > 0.1:
                    recommendations.append("å‘é‡æ£€ç´¢é€Ÿåº¦å¯è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        if "end_to_end_rag" in self.evaluation_results:
            rag_result = self.evaluation_results["end_to_end_rag"]
            if rag_result.get("status") == "success":
                if rag_result.get("avg_response_time", 0) > 10:
                    recommendations.append("RAGå“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æ£€ç´¢å’Œç”Ÿæˆæµç¨‹")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œå»ºè®®æŒç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡")
        
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        # è¿”å›å®Œæ•´æŠ¥å‘Š
        report = {
            "evaluation_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_document": self.test_document,
            "overall_score": overall_score,
            "tests_passed": passed_tests,
            "total_tests": total_tests,
            "performance_metrics": performance_metrics,
            "recommendations": recommendations,
            "detailed_results": self.evaluation_results
        }
        
        return report
    
    def run_evaluation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹RAGç³»ç»Ÿç»¼åˆè¯„ä¼°")
        print(f"ğŸ“… è¯„ä¼°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“„ æµ‹è¯•æ–‡æ¡£: {self.test_document}")
        
        start_time = time.time()
        
        # æ‰§è¡Œå„é¡¹æµ‹è¯•
        tests = [
            ("æ–‡æ¡£åŠ è½½", self.load_test_document),
            ("æ–‡æœ¬åˆ†å—", self.test_text_chunking),
            ("æ„å›¾è¯†åˆ«", self.test_intent_recognition),
            ("é—®ç­”ç”Ÿæˆ", self.test_qa_generation),
            ("å‘é‡æ£€ç´¢", self.test_vector_retrieval),
            ("ç«¯åˆ°ç«¯RAG", self.test_end_to_end_rag),
        ]
        
        for test_name, test_func in tests:
            try:
                success = test_func()
                if not success:
                    print(f"âš ï¸ {test_name}æµ‹è¯•æœªå®Œå…¨é€šè¿‡")
            except Exception as e:
                print(f"âŒ {test_name}æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        
        total_time = time.time() - start_time
        print(f"\nâ±ï¸ æ€»è¯„ä¼°è€—æ—¶: {total_time:.2f} ç§’")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = self.generate_comprehensive_report()
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    evaluator = RAGSystemEvaluator()
    report = evaluator.run_evaluation()
    
    # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    report_file = f"rag_evaluation_report_{int(time.time())}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    return report

if __name__ == "__main__":
    main() 