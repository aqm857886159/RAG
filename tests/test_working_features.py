#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å·¥ä½œåŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•å½“å‰å®Œå…¨å¯ç”¨çš„RAGåŠŸèƒ½
"""

import os
import sys
import logging
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# è®¾ç½®APIå¯†é’¥
os.environ["DEEPSEEK_API_KEY"] = "sk-06810fb5453e4fd1b39e3e5f566da210"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_document_processing():
    """æµ‹è¯•æ–‡æ¡£å¤„ç†åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ“„ æµ‹è¯•æ–‡æ¡£å¤„ç†åŠŸèƒ½")
    print("="*60)
    
    try:
        from document_loader import DocumentLoader
        from text_splitter import get_text_splitter
        
        # åˆå§‹åŒ–ç»„ä»¶
        loader = DocumentLoader()
        splitter = get_text_splitter("recursive", chunk_size=500, chunk_overlap=50)
        
        # æµ‹è¯•æ–‡æ¡£åŠ è½½
        test_file = "docs/test_document.txt"
        if Path(test_file).exists():
            docs = loader.load_single_document(test_file)
            print(f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸ: {len(docs)} ä¸ªç‰‡æ®µ")
            
            # æµ‹è¯•æ–‡æœ¬åˆ†å‰²
            chunks = splitter.split_documents(docs)
            print(f"âœ… æ–‡æœ¬åˆ†å‰²æˆåŠŸ: {len(chunks)} ä¸ªæ–‡æœ¬å—")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æœ¬å—å†…å®¹
            if chunks:
                print(f"ğŸ“„ ç¬¬ä¸€ä¸ªæ–‡æœ¬å—å†…å®¹: {chunks[0].page_content[:200]}...")
            
            return True
        else:
            print(f"âŒ æµ‹è¯•æ–‡æ¡£ä¸å­˜åœ¨: {test_file}")
            return False
            
    except Exception as e:
        print(f"âŒ æ–‡æ¡£å¤„ç†æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_simple_vector_storage():
    """æµ‹è¯•ç®€å•å‘é‡å­˜å‚¨åŠŸèƒ½ï¼ˆä½¿ç”¨å†…å­˜å­˜å‚¨ï¼‰"""
    print("\n" + "="*60)
    print("ğŸ—„ï¸ æµ‹è¯•ç®€å•å‘é‡å­˜å‚¨åŠŸèƒ½")
    print("="*60)
    
    try:
        from vectorizer import SimpleMemoryVectorStore
        from sentence_transformers import SentenceTransformer
        from langchain.docstore.document import Document
        
        # åˆ›å»ºSentenceTransformeré€‚é…å™¨ç±»
        class SentenceTransformerAdapter:
            def __init__(self, model):
                self.model = model
                
            def embed_documents(self, texts):
                """é€‚é…langchainæ¥å£ï¼šæ‰¹é‡åµŒå…¥æ–‡æ¡£"""
                return self.model.encode(texts).tolist()
                
            def embed_query(self, text):
                """é€‚é…langchainæ¥å£ï¼šåµŒå…¥æŸ¥è¯¢"""
                return self.model.encode([text])[0].tolist()
        
        # åˆå§‹åŒ–ç®€å•åµŒå…¥æ¨¡å‹
        print("ğŸ”„ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        # ä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„åµŒå…¥æ¨¡å‹
        try:
            base_model = SentenceTransformer('all-MiniLM-L6-v2')
            embeddings = SentenceTransformerAdapter(base_model)
            print("âœ… ä½¿ç”¨ all-MiniLM-L6-v2 åµŒå…¥æ¨¡å‹")
        except:
            print("âš ï¸ æ— æ³•åŠ è½½SentenceTransformerï¼Œè·³è¿‡å‘é‡å­˜å‚¨æµ‹è¯•")
            return True  # ä¸ç®—å¤±è´¥ï¼Œåªæ˜¯è·³è¿‡
        
        # åˆå§‹åŒ–ç®€å•å‘é‡å­˜å‚¨
        vector_store = SimpleMemoryVectorStore(embeddings)
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        test_docs = [
            Document(page_content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯", metadata={"source": "test1"}),
            Document(page_content="æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ", metadata={"source": "test2"}),
            Document(page_content="è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯AIçš„é‡è¦åº”ç”¨é¢†åŸŸ", metadata={"source": "test3"})
        ]
        
        # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
        print("ğŸ”„ æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨...")
        start_time = time.time()
        vector_store.add_documents(test_docs)
        build_time = time.time() - start_time
        
        print(f"âœ… å‘é‡å­˜å‚¨æ„å»ºæˆåŠŸ: {len(test_docs)} ä¸ªæ–‡æ¡£ï¼Œè€—æ—¶ {build_time:.2f} ç§’")
        
        # æµ‹è¯•æ£€ç´¢
        query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
        print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
        
        start_time = time.time()
        results = vector_store.similarity_search(query, k=2)
        search_time = time.time() - start_time
        
        print(f"âœ… å‘é‡æ£€ç´¢æˆåŠŸ: æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œè€—æ—¶ {search_time:.3f} ç§’")
        
        # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
        for i, doc in enumerate(results):
            print(f"ğŸ“„ ç»“æœ {i+1}: {doc.page_content}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‘é‡å­˜å‚¨æµ‹è¯•å¤±è´¥: {str(e)}")
        print("âš ï¸ è¿™å¯èƒ½æ˜¯ç”±äºåµŒå…¥æ¨¡å‹ä¸‹è½½é—®é¢˜ï¼Œä¸å½±å“æ ¸å¿ƒåŠŸèƒ½")
        return True  # ä¸ç®—å…³é”®å¤±è´¥

def test_generation_system():
    """æµ‹è¯•ç”Ÿæˆç³»ç»ŸåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ¤– æµ‹è¯•ç”Ÿæˆç³»ç»ŸåŠŸèƒ½")
    print("="*60)
    
    try:
        from generator import get_generator
        from src.llm import get_llm
        
        # ç¡®ä¿ä½¿ç”¨DeepSeek LLM
        llm = get_llm(provider="deepseek", api_key="sk-06810fb5453e4fd1b39e3e5f566da210")
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼ˆä¸ä½¿ç”¨RAGï¼‰
        generator = get_generator(use_rag=False, llm=llm)
        
        # æµ‹è¯•ç®€å•ç”Ÿæˆ
        prompt = "è¯·ç®€å•è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
        print(f"ğŸ”„ ç”ŸæˆæŸ¥è¯¢: {prompt}")
        
        start_time = time.time()
        result = generator.generate(prompt)
        gen_time = time.time() - start_time
        
        print(f"âœ… æ–‡æœ¬ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶ {gen_time:.2f} ç§’")
        print(f"ğŸ“ ç”Ÿæˆå†…å®¹: {result['answer'][:300]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆç³»ç»Ÿæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_intent_recognition():
    """æµ‹è¯•æ„å›¾è¯†åˆ«åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ§  æµ‹è¯•æ„å›¾è¯†åˆ«åŠŸèƒ½")
    print("="*60)
    
    try:
        from intent_recognizer import get_intent_recognizer
        
        # åˆå§‹åŒ–æ„å›¾è¯†åˆ«å™¨
        recognizer = get_intent_recognizer(llm_provider="deepseek")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•è®­ç»ƒç¥ç»ç½‘ç»œï¼Ÿ"
        ]
        
        for query in test_queries:
            print(f"\nğŸ”„ æµ‹è¯•æŸ¥è¯¢: {query}")
            
            start_time = time.time()
            result = recognizer.recognize_intent(query)
            rec_time = time.time() - start_time
            
            print(f"âœ… æ„å›¾: {result['intent']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
            print(f"â±ï¸ è€—æ—¶: {rec_time:.2f} ç§’")
            
            # è·å–æ£€ç´¢ç­–ç•¥
            strategy = recognizer.get_retrieval_strategy(result['intent'])
            print(f"ğŸ¯ æ£€ç´¢ç­–ç•¥: top_k={strategy['top_k']}, vector_weight={strategy['vector_weight']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ„å›¾è¯†åˆ«æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_qa_generation():
    """æµ‹è¯•é—®ç­”å¯¹ç”ŸæˆåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ’¬ æµ‹è¯•é—®ç­”å¯¹ç”ŸæˆåŠŸèƒ½")
    print("="*60)
    
    try:
        from data_converter import DocumentConverter
        from langchain.docstore.document import Document
        
        # åˆå§‹åŒ–æ–‡æ¡£è½¬æ¢å™¨
        converter = DocumentConverter(
            llm_provider="deepseek",
            api_key="sk-06810fb5453e4fd1b39e3e5f566da210"
        )
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        test_content = """
        äººå·¥æ™ºèƒ½åŸºç¡€çŸ¥è¯†
        
        äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œç›®æ ‡æ˜¯åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
        ä¸»è¦åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰æŠ€æœ¯é¢†åŸŸã€‚
        """
        
        docs = [Document(page_content=test_content, metadata={"source": "ai_basics"})]
        
        # åˆ†å‰²æ–‡æœ¬
        chunks = converter.text_splitter.split_documents(docs)
        print(f"ğŸ“„ æ–‡æœ¬åˆ†å‰²: {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # ç”Ÿæˆé—®ç­”å¯¹
        print("ğŸ”„ ç”Ÿæˆé—®ç­”å¯¹...")
        start_time = time.time()
        qa_pairs = converter._generate_qa_pairs(chunks)
        gen_time = time.time() - start_time
        
        print(f"âœ… é—®ç­”å¯¹ç”ŸæˆæˆåŠŸ: {len(qa_pairs)} ä¸ªï¼Œè€—æ—¶ {gen_time:.2f} ç§’")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„é—®ç­”å¯¹
        for i, qa in enumerate(qa_pairs):
            print(f"\nQ{i+1}: {qa.question}")
            print(f"A{i+1}: {qa.answer}")
        
        return len(qa_pairs) > 0
        
    except Exception as e:
        print(f"âŒ é—®ç­”å¯¹ç”Ÿæˆæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_streamlit_app_status():
    """æµ‹è¯•Streamlitåº”ç”¨çŠ¶æ€"""
    print("\n" + "="*60)
    print("ğŸŒ æµ‹è¯•Streamlitåº”ç”¨çŠ¶æ€")
    print("="*60)
    
    try:
        import requests
        
        # æ£€æŸ¥åº”ç”¨æ˜¯å¦åœ¨è¿è¡Œ
        try:
            response = requests.get("http://localhost:8501", timeout=5)
            if response.status_code == 200:
                print("âœ… Streamlitåº”ç”¨æ­£å¸¸è¿è¡Œ")
                print("ğŸŒ è®¿é—®åœ°å€: http://localhost:8501")
                return True
            else:
                print(f"âš ï¸ Streamlitåº”ç”¨å“åº”å¼‚å¸¸: {response.status_code}")
                return False
        except requests.exceptions.ConnectionError:
            print("âŒ æ— æ³•è¿æ¥åˆ°Streamlitåº”ç”¨")
            print("ğŸ’¡ è¯·ç¡®ä¿åº”ç”¨æ­£åœ¨è¿è¡Œ: streamlit run src/app.py")
            return False
        except requests.exceptions.Timeout:
            print("âš ï¸ Streamlitåº”ç”¨å“åº”è¶…æ—¶")
            return False
            
    except ImportError:
        print("âš ï¸ æœªå®‰è£…requestsåº“ï¼Œè·³è¿‡ç½‘ç»œè¿æ¥æµ‹è¯•")
        # æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
        import socket
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex(('localhost', 8501))
        sock.close()
        
        if result == 0:
            print("âœ… ç«¯å£8501æ­£åœ¨è¢«ä½¿ç”¨ï¼ˆå¯èƒ½æ˜¯Streamlitåº”ç”¨ï¼‰")
            print("ğŸŒ è®¿é—®åœ°å€: http://localhost:8501")
            return True
        else:
            print("âŒ ç«¯å£8501æœªè¢«ä½¿ç”¨ï¼Œåº”ç”¨å¯èƒ½æœªå¯åŠ¨")
            return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å·¥ä½œåŠŸèƒ½æµ‹è¯•")
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nğŸ¯ æµ‹è¯•è¯´æ˜: ä¸“é—¨æµ‹è¯•å½“å‰å®Œå…¨å¯ç”¨çš„åŠŸèƒ½")
    
    # æµ‹è¯•é¡¹ç›®åˆ—è¡¨
    tests = [
        ("æ–‡æ¡£å¤„ç†", test_document_processing),
        ("ç®€å•å‘é‡å­˜å‚¨", test_simple_vector_storage),
        ("ç”Ÿæˆç³»ç»Ÿ", test_generation_system),
        ("æ„å›¾è¯†åˆ«", test_intent_recognition),
        ("é—®ç­”å¯¹ç”Ÿæˆ", test_qa_generation),
        ("Streamlitåº”ç”¨çŠ¶æ€", test_streamlit_app_status),
    ]
    
    results = []
    start_time = time.time()
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å‡ºç°å¼‚å¸¸: {str(e)}")
            results.append((test_name, False))
    
    # æ€»ç»“æµ‹è¯•ç»“æœ
    total_time = time.time() - start_time
    print("\n" + "="*80)
    print("ğŸ“Š å·¥ä½œåŠŸèƒ½æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*80)
    
    passed = 0
    total = len(results)
    critical_passed = 0  # å…³é”®åŠŸèƒ½é€šè¿‡æ•°é‡
    critical_tests = ["æ–‡æ¡£å¤„ç†", "ç”Ÿæˆç³»ç»Ÿ", "æ„å›¾è¯†åˆ«", "é—®ç­”å¯¹ç”Ÿæˆ"]
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:20} : {status}")
        if result:
            passed += 1
            if test_name in critical_tests:
                critical_passed += 1
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡ ({passed/total*100:.1f}%)")
    print(f"å…³é”®åŠŸèƒ½: {critical_passed}/{len(critical_tests)} é¡¹é€šè¿‡ ({critical_passed/len(critical_tests)*100:.1f}%)")
    print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
    
    # åˆ¤æ–­ç³»ç»ŸçŠ¶æ€
    if critical_passed == len(critical_tests):
        print("\nğŸ‰ æ‰€æœ‰å…³é”®åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼")
        print("ğŸŒŸ RAGç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½å®Œå…¨å¯ç”¨ï¼")
        
        if passed == total:
            print("ğŸš€ æ‰€æœ‰åŠŸèƒ½éƒ½æ­£å¸¸å·¥ä½œï¼Œç³»ç»Ÿå®Œç¾è¿è¡Œï¼")
        else:
            print("âš ï¸ éƒ¨åˆ†è¾…åŠ©åŠŸèƒ½æœ‰é—®é¢˜ï¼Œä½†ä¸å½±å“æ ¸å¿ƒä½¿ç”¨")
    else:
        print(f"\nâš ï¸ {len(critical_tests)-critical_passed} é¡¹å…³é”®åŠŸèƒ½å¤±è´¥")
        print("ğŸ”§ å»ºè®®ä¼˜å…ˆä¿®å¤å…³é”®åŠŸèƒ½")
        
    return critical_passed == len(critical_tests)

if __name__ == "__main__":
    main() 