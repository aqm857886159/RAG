#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®Œæ•´ç³»ç»Ÿæµ‹è¯•è„šæœ¬
æµ‹è¯•RAGç³»ç»Ÿçš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
"""

import os
import sys
import logging
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# è®¾ç½®APIå¯†é’¥
os.environ["DEEPSEEK_API_KEY"] = "sk-06810fb5453e4fd1b39e3e5f566da210"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_document_processing():
    """æµ‹è¯•æ–‡æ¡£å¤„ç†åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ“„ æµ‹è¯•æ–‡æ¡£å¤„ç†åŠŸèƒ½")
    print("="*60)
    
    try:
        from document_loader import DocumentLoader
        from text_splitter import get_text_splitter
        
        # åˆå§‹åŒ–ç»„ä»¶
        loader = DocumentLoader()
        splitter = get_text_splitter("recursive", chunk_size=500, chunk_overlap=50)
        
        # æµ‹è¯•æ–‡æ¡£åŠ è½½
        test_file = "docs/test_document.txt"
        if Path(test_file).exists():
            docs = loader.load_single_document(test_file)
            print(f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸ: {len(docs)} ä¸ªç‰‡æ®µ")
            
            # æµ‹è¯•æ–‡æœ¬åˆ†å‰²
            chunks = splitter.split_documents(docs)
            print(f"âœ… æ–‡æœ¬åˆ†å‰²æˆåŠŸ: {len(chunks)} ä¸ªæ–‡æœ¬å—")
            
            return True
        else:
            print(f"âŒ æµ‹è¯•æ–‡æ¡£ä¸å­˜åœ¨: {test_file}")
            return False
            
    except Exception as e:
        print(f"âŒ æ–‡æ¡£å¤„ç†æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_vector_storage():
    """æµ‹è¯•å‘é‡å­˜å‚¨åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ—„ï¸ æµ‹è¯•å‘é‡å­˜å‚¨åŠŸèƒ½")
    print("="*60)
    
    try:
        from vectorizer import get_vectorizer
        from document_loader import DocumentLoader
        from text_splitter import get_text_splitter
        
        # åˆå§‹åŒ–ç»„ä»¶ - ä¿®å¤å‚æ•°ä¼ é€’
        vectorizer = get_vectorizer(
            embedding_model_name="BAAI/bge-small-zh-v1.5",
            vector_store_type="faiss"
        )
        loader = DocumentLoader()
        splitter = get_text_splitter("recursive", chunk_size=500, chunk_overlap=50)
        
        # åŠ è½½æµ‹è¯•æ–‡æ¡£
        test_file = "docs/test_document.txt"
        if Path(test_file).exists():
            docs = loader.load_single_document(test_file)
            chunks = splitter.split_documents(docs)
            
            # æ„å»ºå‘é‡æ•°æ®åº“
            start_time = time.time()
            vectorizer.add_documents(chunks)
            build_time = time.time() - start_time
            
            print(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºæˆåŠŸ: {len(chunks)} ä¸ªæ–‡æ¡£ï¼Œè€—æ—¶ {build_time:.2f} ç§’")
            
            # æµ‹è¯•æ£€ç´¢
            query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
            start_time = time.time()
            results = vectorizer.similarity_search(query, k=3)
            search_time = time.time() - start_time
            
            print(f"âœ… å‘é‡æ£€ç´¢æˆåŠŸ: æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œè€—æ—¶ {search_time:.3f} ç§’")
            
            return True
        else:
            print(f"âŒ æµ‹è¯•æ–‡æ¡£ä¸å­˜åœ¨: {test_file}")
            return False
            
    except Exception as e:
        print(f"âŒ å‘é‡å­˜å‚¨æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_retrieval_system():
    """æµ‹è¯•æ£€ç´¢ç³»ç»ŸåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ” æµ‹è¯•æ£€ç´¢ç³»ç»ŸåŠŸèƒ½")
    print("="*60)
    
    try:
        from retriever import get_retriever
        from vectorizer import get_vectorizer
        from document_loader import DocumentLoader
        from text_splitter import get_text_splitter
        
        # åˆå§‹åŒ–ç»„ä»¶ - ä¿®å¤å‚æ•°ä¼ é€’
        vectorizer = get_vectorizer(
            embedding_model_name="BAAI/bge-small-zh-v1.5",
            vector_store_type="faiss"
        )
        retriever = get_retriever("hybrid", vectorizer=vectorizer)
        loader = DocumentLoader()
        splitter = get_text_splitter("recursive", chunk_size=500, chunk_overlap=50)
        
        # å‡†å¤‡æ•°æ®
        test_file = "docs/test_document.txt"
        if Path(test_file).exists():
            docs = loader.load_single_document(test_file)
            chunks = splitter.split_documents(docs)
            vectorizer.add_documents(chunks)
            
            # æµ‹è¯•æ£€ç´¢
            query = "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„åŒºåˆ«"
            start_time = time.time()
            results = retriever.get_relevant_documents(query, top_k=3)
            search_time = time.time() - start_time
            
            print(f"âœ… æ··åˆæ£€ç´¢æˆåŠŸ: æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œè€—æ—¶ {search_time:.3f} ç§’")
            
            # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
            for i, doc in enumerate(results[:2]):
                print(f"\nğŸ“„ ç»“æœ {i+1}: {doc.page_content[:100]}...")
            
            return True
        else:
            print(f"âŒ æµ‹è¯•æ–‡æ¡£ä¸å­˜åœ¨: {test_file}")
            return False
            
    except Exception as e:
        print(f"âŒ æ£€ç´¢ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_generation_system():
    """æµ‹è¯•ç”Ÿæˆç³»ç»ŸåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ¤– æµ‹è¯•ç”Ÿæˆç³»ç»ŸåŠŸèƒ½")
    print("="*60)
    
    try:
        from generator import get_generator
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = get_generator("deepseek", api_key="sk-06810fb5453e4fd1b39e3e5f566da210")
        
        # æµ‹è¯•ç®€å•ç”Ÿæˆ
        prompt = "è¯·ç®€å•è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
        start_time = time.time()
        response = generator.llm(prompt)
        gen_time = time.time() - start_time
        
        print(f"âœ… æ–‡æœ¬ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶ {gen_time:.2f} ç§’")
        print(f"ğŸ“ ç”Ÿæˆå†…å®¹: {response[:200]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆç³»ç»Ÿæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_complete_rag_pipeline():
    """æµ‹è¯•å®Œæ•´RAGæµæ°´çº¿"""
    print("\n" + "="*60)
    print("ğŸ”„ æµ‹è¯•å®Œæ•´RAGæµæ°´çº¿")
    print("="*60)
    
    try:
        from document_loader import DocumentLoader
        from text_splitter import get_text_splitter
        from vectorizer import get_vectorizer
        from retriever import get_retriever
        from generator import get_generator
        from intent_recognizer import get_intent_recognizer
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶ - ä¿®å¤å‚æ•°ä¼ é€’
        loader = DocumentLoader()
        splitter = get_text_splitter("recursive", chunk_size=500, chunk_overlap=50)
        vectorizer = get_vectorizer(
            embedding_model_name="BAAI/bge-small-zh-v1.5",
            vector_store_type="faiss"
        )
        retriever = get_retriever("hybrid", vectorizer=vectorizer)
        generator = get_generator("deepseek", api_key="sk-06810fb5453e4fd1b39e3e5f566da210")
        intent_recognizer = get_intent_recognizer("deepseek")
        
        # 1. æ–‡æ¡£å¤„ç†
        test_file = "docs/test_document.txt"
        if not Path(test_file).exists():
            print(f"âŒ æµ‹è¯•æ–‡æ¡£ä¸å­˜åœ¨: {test_file}")
            return False
            
        docs = loader.load_single_document(test_file)
        chunks = splitter.split_documents(docs)
        vectorizer.add_documents(chunks)
        print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # 2. æµ‹è¯•æŸ¥è¯¢
        query = "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
        print(f"\nğŸ¤” ç”¨æˆ·æŸ¥è¯¢: {query}")
        
        # 3. æ„å›¾è¯†åˆ«
        intent_result = intent_recognizer.recognize_intent(query)
        print(f"ğŸ§  æ„å›¾è¯†åˆ«: {intent_result['intent']} (ç½®ä¿¡åº¦: {intent_result['confidence']:.3f})")
        
        # 4. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        strategy = intent_recognizer.get_retrieval_strategy(intent_result['intent'])
        relevant_docs = retriever.get_relevant_documents(query, top_k=strategy['top_k'])
        print(f"ğŸ” æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # 5. æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        # 6. ç”Ÿæˆå›ç­”
        template = intent_recognizer.get_prompt_template(intent_result['intent'])
        prompt = template.format(context=context, question=query)
        
        start_time = time.time()
        answer = generator.llm(prompt)
        gen_time = time.time() - start_time
        
        print(f"\nğŸ’¡ RAGå›ç­” (è€—æ—¶ {gen_time:.2f} ç§’):")
        print(f"{answer}")
        
        print(f"\nâœ… å®Œæ•´RAGæµæ°´çº¿æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´RAGæµæ°´çº¿æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å®Œæ•´ç³»ç»Ÿæµ‹è¯•")
    print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æµ‹è¯•é¡¹ç›®åˆ—è¡¨
    tests = [
        ("æ–‡æ¡£å¤„ç†", test_document_processing),
        ("å‘é‡å­˜å‚¨", test_vector_storage),
        ("æ£€ç´¢ç³»ç»Ÿ", test_retrieval_system),
        ("ç”Ÿæˆç³»ç»Ÿ", test_generation_system),
        ("å®Œæ•´RAGæµæ°´çº¿", test_complete_rag_pipeline),
    ]
    
    results = []
    start_time = time.time()
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å‡ºç°å¼‚å¸¸: {str(e)}")
            results.append((test_name, False))
    
    # æ€»ç»“æµ‹è¯•ç»“æœ
    total_time = time.time() - start_time
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*80)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:20} : {status}")
        if result:
            passed += 1
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡ ({passed/total*100:.1f}%)")
    print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼")
        print("ğŸŒŸ RAGç³»ç»Ÿå·²å®Œå…¨ä¿®å¤å¹¶å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
    else:
        print(f"\nâš ï¸  {total-passed} é¡¹æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
        
    return passed == total

if __name__ == "__main__":
    main() 